<!DOCTYPE html>
<html lang="en" class="fontawesome-i2svg-active fontawesome-i2svg-complete">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Rosetta Neurons: Mining the Common Units in a Model Zoo</title>
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6">
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
      <script defer="" src="css/fontawesome.all.min.js"></script>
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <style>
      body {
        font-size:20px;
        margin:60px auto;
        width:auto;
        max-width:900px;
      }

      hr {
        border:0;
        height:1.0px;
        background-image:linear-gradient(to right, rgba(0, 0, 0, 0.3), rgba(0, 0, 0, 0.3), rgba(0, 0, 0, 0.3));
      }

      .gap-30 {
      width:100%;
      height:30px;
      }

      .gap-20 {
      width:100%;
      height:20px;
      }

      .gap-10 {
      width:100%;
      height:10px;
      }

      .gap-5 {
      width:100%;
      height:5px;
      }
      .paper {
        max-width: 700px;
      }
      @media (max-width: 910px) {
        .paper {
          max-width: 500px;
        }
      }
      @media (max-width: 610px) {
        .paper {
          max-width: 300px;
        }
      }
    </style>
  </head>

  <div class="container">
  <body>

    <center><span style="font-size:40px">
      Rosetta Neurons: <br>Mining the Common Units in a Model Zoo
    </span></center>
    <div class="gap-20"></div>

    <!---------------------  authors --------------------->
    <span style="font-size:20px">
    <div class="row">

      <div class="col-md-3">
        <center><a href="https://avdravid.github.io"><span style="font-size:21px">Amil Dravid*</span></a>
          <sup style="font-size:15px">1</sup></center>
      </div>
      <div class="col-md-3">
        <center><a href="https://yossi.gandelsman.com"><span style="font-size:21px">Yossi Gandelsman*</span></a>
          <sup style="font-size:15px">2</sup></center>
      </div>
      <div class="col-md-3">
        <center><a href="https://people.eecs.berkeley.edu/~efros/"><span style="font-size:21px">Alexei A. Efros</span></a>
          <sup style="font-size:15px">2</sup></center>
      </div>
      <div class="col-md-3">
        <center><a href="https://assafshocher.github.io"><span style="font-size:21px">Assaf Shocher</span></a>
          <sup style="font-size:15px">23</sup></center>
      </div>
    </div>
  </span>
    <!-- <center>
    <span style="font-size:20px">
          &nbsp;
          <div style="display:inline-block">
          <a href="https://avdravid.github.io">Amil Dravid*</a>
          <sup style="font-size:15px">1</sup>,
          </div>
          &nbsp;
          <div style="display:inline-block">
          <a href="https://yossi.gandelsman.com">Yossi Gandelsman*</a>
          <sup style="font-size:15px">2</sup>,
          </div>
          &nbsp;
          <div style="display:inline-block">
          <a href="https://assafshocher.github.io">Assaf Shocher</a>
          <sup style="font-size:15px">23</sup>,
          </div>
          &nbsp;
          <div style="display:inline-block">
          <a href="https://people.eecs.berkeley.edu/~efros/">Alexei A. Efros</a>
          <sup style="font-size:15px">2</sup>
          </div>
    </span>
	</center> -->
    <!--------------------- affiliations --------------------->
    <div class="gap-5"></div>
    
    <div class="row">

      <div class="col-md-4">
        <center><span style="font-size:18px"><sup style="font-size:15px">1</sup>
          Northwestern
        </span></center>
      </div>
      <div class="col-md-4">
        <center><span style="font-size:18px"><sup style="font-size:15px">2</sup>
          UC Berkeley
        </span></center>
      </div>
      <div class="col-md-4">
        <center><span style="font-size:18px"><sup style="font-size:15px">3</sup>
          Google Research
        </span></center>
      </div>
    </div>
    <div class="gap-10"></div>
    <p>* Equal contribution</p>
    <hr>

    <!--------------------- links --------------------->
    <div class="gap-10"></div>

    <center>
    <span style="font-size:23px"> 
      <b>ICCV 2023</b>
      &nbsp; 
      [<a href="https://arxiv.org/abs/2306.09346" class="external-link button is-normal is-rounded is-dark">paper</a>]
      &nbsp;
      [<a href="bibtex.txt">BibTeX</a>]
      &nbsp; 
      [<a href="https://github.com/yossigandelsman/rosetta_neurons">code</a>]
    </span>
    <div class="gap-20"></div>

    </center>
<img width="100%" src="images/teaser.png">
<p></p>
<p><i><b>Mining for "Rosetta Neurons".</b> Our findings demonstrate the existence of matching neurons across different models that express a shared concept (such as object contours, object parts, and colors). These concepts emerge without any supervision or manual annotations. We visualize the concepts with heatmaps and a novel inversion technique (two right columns).
  </i></p>
<hr>
    <!--------------------- abstract --------------------->
    <div class="gap-20"></div>
    <center><b><span style="font-size:25px">Abstract</span></b><br>
    <div class="gap-10"></div>
    <p> 
    Do different neural networks, trained for various vision tasks, share some common representations?
   In this paper, we demonstrate the existence of common features we call <i>"Rosetta Neurons"</i> across a range of models with different architectures, different tasks (generative and discriminative), and different types of supervision (class-supervised, text-supervised, self-supervised). We present an algorithm for mining a dictionary of Rosetta Neurons across several popular vision models: <a href="https://arxiv.org/abs/1512.03385">Class Supervised-ResNet50</a>, <a href="https://arxiv.org/abs/2104.14294">DINO-ResNet50</a>, <a href="https://arxiv.org/abs/2104.14294">DINO-ViT</a>, <a href="https://arxiv.org/abs/2111.06377">MAE</a>, <a href="https://arxiv.org/abs/2103.00020">CLIP-ResNet50</a>, <a href="https://arxiv.org/abs/1809.11096">BigGAN</a>, <a href="https://arxiv.org/abs/1912.04958">StyleGAN-2</a>, <a href="https://arxiv.org/abs/2202.00273">StyleGAN-XL</a>.

   Our findings suggest that certain visual concepts and structures are inherently embedded in the natural world and can be learned by different models regardless of the specific task or architecture, and without the use of semantic labels. We can visualize shared concepts directly due to generative models included in our analysis. The Rosetta Neurons facilitate model-to-model translation enabling various inversion-based manipulations, including cross-class alignments, shifting, zooming, and more, without the need for specialized training.
    </p>
    </center>
    <hr>
    <!--------------------- content --------------------->
    <div class="gap-20"></div>
    <b><span style="font-size:25px">Rosetta Neurons guided image inversion</span></b>
    <br>
    <center>
    <img class="paper" src="images/inversion_architecture.png" alt="">
    </center>
    <p>An input image is passed through a discriminative model <i>D</i> (i.e.: DINO) to obtain
the Rosetta Neuronsâ€™ activation maps. Then, the latent code <i>Z</i> of the generator is optimized to match those activation maps, according to
the extracted pairs.</p>
    
    <p>By incorporating
the Rosetta Neurons in the image inversion process, we can invert sketches and cartoons (first row), and generate similar in-distribution images (last row):</p>
<center><img class="paper" src="images/cartoon.png" alt="" style="width: 500px"></center>
<p>A subset of the Rosetta Neurons
from the input images that were matched during the inversion process is shown in the middle rows.</p>
  <hr>
    <b><span style="font-size:25px">Rosetta Neurons guided editing</span></b>

    <div class="gap-20"></div>
    <p>Direct manipulations on the activation maps corresponding to the Rosetta neurons are translated to manipulations in the image space. We use two models (top row - StyleGAN2, bottom two rows - BigGAN) and utilize the matches between each of them to DINO-RN:</p>
    <center><img class="paper" src="images/editing.png" alt=""></center>
    <hr>
    <b><span style="font-size:25px">Acknowledgements</span></b>
  <div class="gap-20"></div>
    <p>The authors would like to thank Niv Haim, Bill Peebles, Sasha Sax, Karttikeya Mangalam and Xinlei Chen for the helpful discussions. YG is funded by the Berkeley Fellowship. AS gratefully acknowledges financial support for this publication by the Fulbright U.S. Postdoctoral Program, which is sponsored by the U.S. Department of State. Its contents are solely the responsibility of the author and do not necessarily represent the official views of the Fulbright Program or the Government of the United States.
Additional funding came from DARPA MCS and ONR MURI.</p>
  </body>
  </div>

  <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
  <!-- Include all compiled plugins (below), or include individual files as needed -->
  <script src="js/bootstrap.min.js"></script>

</html>